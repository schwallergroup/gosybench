{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf02fd5-681f-43fd-92ef-5151dabda1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils import gen_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6affeab9-128a-446d-a492-351561ec7923",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing\n",
    "\n",
    "We'll merge here data from multiple sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca9ad5f4-89b7-426d-bd6d-fc9a683e40b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3748163\n",
      "1878060\n",
      "1872064\n",
      "CPU times: user 58.5 s, sys: 4.78 s, total: 1min 3s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Load original uspto csv with all fields.\n",
    "uspto_csv_1 = pd.read_csv('../../data/raw/Extracted_Data_2001_Sep2016_USPTOapplications_new.csv', low_memory=False)\n",
    "uspto_csv_2 = pd.read_csv('../../data/raw/Extracted_Data_1976_Sep2016_USPTOgrants_new.csv', low_memory=False)\n",
    "uspto_csv = pd.concat([uspto_csv_1, uspto_csv_2])\n",
    "\n",
    "print(len(uspto_csv))\n",
    "\n",
    "# Load original paragraphs DB\n",
    "with open('../../data/raw/DATASET_PARAGRAPH_Q2_Q3.pickle', 'rb') as f:\n",
    "    parags_db = pickle.load(f)\n",
    "    print(len(parags_db))\n",
    "\n",
    "# Load segmented paragraphs\n",
    "with open('../../data/processed/uspto_segmented_last.bin', 'rb') as f:\n",
    "    segm_db = pickle.load(f)\n",
    "    print(len(segm_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b53d6-6c12-40a1-bf68-8a73cbe78450",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Let's merge all this data into a single structure, with additions.\n",
    "\n",
    "```json\n",
    "{\n",
    "    text_id: [\n",
    "        'txt_sgm':\n",
    "        'src_prg':\n",
    "        'sgm_cls':\n",
    "        'stp_ord':\n",
    "        'rxn_smi':\n",
    "        'prd_str':  Reaction product as given in paragraph\n",
    "    ] \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7889fa-8b46-4596-9e30-8980b763b367",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Match paragraphs with reaction SMILES, and sort.\n",
    "\n",
    "This will give us a map `src_prg` -> `rxn_smi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d17410a3-61e4-4731-aedd-e3d382a080af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF shape: (1878060, 3)\n",
      "CPU times: user 15.2 s, sys: 519 ms, total: 15.7 s\n",
      "Wall time: 15.7 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paragraph Text</th>\n",
       "      <th>Reaction Smiles</th>\n",
       "      <th>Product List</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Suspend anhydrous AlCl3 (156 g, 1.15 mol) in t...</td>\n",
       "      <td>[Al+3].[Cl-].[Cl-].[Cl-].[Cl:5][CH2:6][CH2:7][...</td>\n",
       "      <td>['4-Chloro-1-(4-methyl-phenyl)-butan-1-one']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dissolve 4-chloro-1-(4-isopropyl-phenyl)-butan...</td>\n",
       "      <td>[Cl:1][CH2:2][CH2:3][CH2:4][C:5]([C:7]1[CH:12]...</td>\n",
       "      <td>['1-[4-(1-Bromo-1-methyl-ethyl)-phenyl]-4-chlo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mix 2-ethyl-1-hexanol (6.5 g, 5 mol), triethyl...</td>\n",
       "      <td>C(C(CCCC)C[OH:5])C.C(N(CC)CC)C.[C:17]1([CH2:23...</td>\n",
       "      <td>['2-phenylaceticacid', '2-(2-ethylhexy)lester']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Paragraph Text  \\\n",
       "index                                                      \n",
       "0      Suspend anhydrous AlCl3 (156 g, 1.15 mol) in t...   \n",
       "1      Dissolve 4-chloro-1-(4-isopropyl-phenyl)-butan...   \n",
       "2      Mix 2-ethyl-1-hexanol (6.5 g, 5 mol), triethyl...   \n",
       "\n",
       "                                         Reaction Smiles  \\\n",
       "index                                                      \n",
       "0      [Al+3].[Cl-].[Cl-].[Cl-].[Cl:5][CH2:6][CH2:7][...   \n",
       "1      [Cl:1][CH2:2][CH2:3][CH2:4][C:5]([C:7]1[CH:12]...   \n",
       "2      C(C(CCCC)C[OH:5])C.C(N(CC)CC)C.[C:17]1([CH2:23...   \n",
       "\n",
       "                                            Product List  \n",
       "index                                                     \n",
       "0           ['4-Chloro-1-(4-methyl-phenyl)-butan-1-one']  \n",
       "1      ['1-[4-(1-Bromo-1-methyl-ethyl)-phenyl]-4-chlo...  \n",
       "2        ['2-phenylaceticacid', '2-(2-ethylhexy)lester']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prg_rxn_sort = (\n",
    "    parags_db\n",
    "    .reset_index()\n",
    "    .merge(\n",
    "        uspto_csv[['Paragraph Text', 'Reaction Smiles', 'Product List']].drop_duplicates(),\n",
    "        right_on='Paragraph Text',\n",
    "        left_on='Paragraph Text',\n",
    "        how='outer'\n",
    "    )\n",
    "    .dropna()\n",
    "    .drop_duplicates(subset=['index'])\n",
    "    .sort_values('index')\n",
    ")\n",
    "\n",
    "prg_rxn_sort['index'] = prg_rxn_sort['index'].astype(int)\n",
    "prg_rxn_sort = prg_rxn_sort.set_index('index')\n",
    "\n",
    "print(f\"DF shape: {prg_rxn_sort.shape}\")\n",
    "prg_rxn_sort.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cae7b0-df93-4aaa-b375-90ca926135dd",
   "metadata": {},
   "source": [
    "## First, clean duplicates in segm_db and paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "598d7eb1-088b-4dfb-873b-b3c4b6a78380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1872064\n",
      "993733\n"
     ]
    }
   ],
   "source": [
    "# Get a list of indices that point ot same paragraph\n",
    "p_indices = prg_rxn_sort.groupby('Paragraph Text').indices\n",
    "\n",
    "# This maps every paragraph index to a unique idx for that paragraph.\n",
    "map_idx = {j:inds[0] for p,inds in p_indices.items() for j in inds}\n",
    "\n",
    "# Apply transform to segm_db\n",
    "print(len(segm_db))\n",
    "segm_db = {map_idx[key]:sg for key, sg in segm_db.items()}\n",
    "print(len(segm_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8dd187-1148-4280-92fe-1aa06a4eb56b",
   "metadata": {},
   "source": [
    "## Build the dict with all merged data as discussed above (sg_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca583857-3a81-4548-8c40-149d17b157ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a61b85ac91c41449673398a41a151c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/993733 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2812027\n",
      "CPU times: user 12.7 s, sys: 82.9 ms, total: 12.8 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sg_db = []\n",
    "rxn_dict = prg_rxn_sort['Reaction Smiles'].to_dict()\n",
    "prd_dict = prg_rxn_sort['Product List'].to_dict()\n",
    "\n",
    "for prg_id, prg in tqdm(segm_db.items()):\n",
    "    for sgmnt in prg:\n",
    "        try:\n",
    "            cls = sgmnt['text class']\n",
    "    \n",
    "            feats = {\n",
    "                'txt_sgm': sgmnt['text segment'].strip(\"'\"),\n",
    "                'src_prg': prg_id,\n",
    "                'sgm_cls': sgmnt['text class'],\n",
    "                'stp_ord': int(sgmnt['step order']),\n",
    "            }\n",
    "            \n",
    "            if cls == 'reaction set-up':\n",
    "                feats['rxn_smi'] = rxn_dict[prg_id]\n",
    "                feats['prd_str'] = ast.literal_eval(prd_dict[prg_id])\n",
    "    \n",
    "            sg_db.append(feats)\n",
    "        except:\n",
    "            sg_db.append(feats)\n",
    "            pass\n",
    "\n",
    "print(len(sg_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448f53f8-f44e-4018-81ff-527f1cefb154",
   "metadata": {},
   "source": [
    "## Now let's filter out by edit distance\n",
    "\n",
    "If edit distance is above some threshold, sample is wrong and we discard it. This way we ensure segmentation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "725edf5c-7f8b-4fe1-a775-45aa9f5263ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit distances of first 10 paragraphs/segmentations: [0 0 0 0 0 1 5 2 1 3]\n",
      "\n",
      "\n",
      " edd    Number of samples\n",
      "\n",
      "   0    453568\n",
      "   5    129884\n",
      "  10    113703\n",
      "  50    104008\n",
      " 100     92754\n",
      " 500     26756\n",
      "1000      3682\n",
      "\n",
      "len of discriminator (should be same as current len of sg_db): 2812027\n",
      "\n",
      "len of sg_db: 2812027\n",
      "\n",
      "CPU times: user 5.08 s, sys: 0 ns, total: 5.08 s\n",
      "Wall time: 5.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from utils import edit_distance_checkor\n",
    "\n",
    "def quality_filter(i):\n",
    "    try:\n",
    "        edd = edit_distance_checkor(\n",
    "            parags_db[i],\n",
    "            segm_db[i]\n",
    "        )\n",
    "        return edd\n",
    "    except:\n",
    "        return 1000\n",
    "\n",
    "\n",
    "edit_distances = np.array([quality_filter(i) for i in segm_db.keys()])\n",
    "print(f\"Edit distances of first 10 paragraphs/segmentations: {edit_distances[:10]}\")\n",
    "\n",
    "print(f\"\\n\\n{'edd':>4}{'Number of samples':>21}\\n\")\n",
    "for t in [0, 5, 10, 50, 100, 500, 1000]:\n",
    "    print(f\"{t:>4}{(edit_distances > t).sum():>10}\")\n",
    "\n",
    "# Let's not consider samples with edd > 10\n",
    "remove_prgs = edit_distances > 10\n",
    "\n",
    "# Recover indices of the segments to know what to clean from embeddings\n",
    "keep_idx = []\n",
    "for j, p in enumerate(segm_db.keys()):\n",
    "    # Simply repeat the value of remove_prgs[p] for each segment\n",
    "    keep_idx += [remove_prgs[j] for i in segm_db[p]]  \n",
    "\n",
    "print(f\"\\nlen of discriminator (should be same as current len of sg_db): {len(keep_idx)}\\n\")\n",
    "print(f\"len of sg_db: {len(sg_db)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde78abf-941a-474e-bc36-bf4b56d4113e",
   "metadata": {},
   "source": [
    "## Finally drop the failed entries and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "230ca094-d5df-49ad-a6e0-a01a3b0e5bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 225300 segments\n",
      "Each entry of our dictionary looks something like this:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'txt_sgm': 'Suspend anhydrous AlCl3 (156 g, 1.15 mol) in toluene (1500 mL) and cool to 2-4° C. Add, by slow addition, a solution of 4-chlorobutyryl chloride (165.5 g, 1.15 mol) in toluene (300 mL). Stir for 15 minutes and pour into stirring ice-water (2.5 L). Stir for 30 hours,',\n",
       " 'src_prg': 0,\n",
       " 'sgm_cls': 'reaction set-up',\n",
       " 'stp_ord': 1,\n",
       " 'rxn_smi': '[Al+3].[Cl-].[Cl-].[Cl-].[Cl:5][CH2:6][CH2:7][CH2:8][C:9](Cl)=[O:10].[C:12]1([CH3:18])[CH:17]=[CH:16][CH:15]=[CH:14][CH:13]=1>>[Cl:5][CH2:6][CH2:7][CH2:8][C:9]([C:15]1[CH:16]=[CH:17][C:12]([CH3:18])=[CH:13][CH:14]=1)=[O:10] |f:0.1.2.3|',\n",
       " 'prd_str': ['4-Chloro-1-(4-methyl-phenyl)-butan-1-one']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_db_clean = [s for i,s in enumerate(sg_db) if not keep_idx[i]]\n",
    "print(f\"Dropping {np.sum(keep_idx)} segments\")\n",
    "\n",
    "with open('../../data/processed/sg_db_clean.bin', 'wb') as f:\n",
    "    pickle.dump(sg_db_clean, f)\n",
    "\n",
    "print(f\"Each entry of our dictionary looks something like this:\\n\")\n",
    "sg_db_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fbb17ba-a05a-4d14-83b6-ac6510e681dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2586727"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sg_db_clean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syn2act",
   "language": "python",
   "name": "syn2act"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
